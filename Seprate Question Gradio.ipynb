{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "import collections\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load the emotion prediction model\n",
    "with open(\"F:\\\\SER DCA\\\\DCA_SER.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"F:\\\\SER DCA\\\\DCA_SER.weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "classes = ['disgust', 'sad', 'fear', 'happy', 'angry', 'neutral', 'surprise']\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit_transform(np.array(classes).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.26.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"1. How have you been feeling lately?\",\n",
    "    \"2. Any changes in your thoughts or emotions?\",\n",
    "    \"3. Found coping strategies helpful?\",\n",
    "    \"4. Specific challenges you're facing?\",\n",
    "    \"5. Implemented discussed techniques?\",\n",
    "    \"6. Notice any improvements in well-being?\",\n",
    "    \"7. Any areas where you feel stuck?\",\n",
    "    \"8. Noticed any patterns or triggers?\",\n",
    "    \"9. Anything else you'd like to discuss?\"\n",
    "]\n",
    "\n",
    "# Create audio input objects with questions as labels\n",
    "audio_file_1 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[0])\n",
    "audio_file_2 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[1])\n",
    "audio_file_3 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[2])\n",
    "audio_file_4 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[3])\n",
    "audio_file_5 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[4])\n",
    "audio_file_6 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[5])\n",
    "audio_file_7 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[6])\n",
    "audio_file_8 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[7])\n",
    "audio_file_9 = gr.Audio(sources=\"microphone\", type=\"numpy\", format='wav', max_length=4.5, label=questions[8])\n",
    "\n",
    "audio_df = [audio_file_1,audio_file_2,audio_file_3,audio_file_4,audio_file_5,audio_file_6,audio_file_7,audio_file_8,audio_file_9]\n",
    "\n",
    "def zcr(data,frame_length,hop_length):\n",
    "    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "def rmse(data,frame_length=2048,hop_length=512):\n",
    "    rmse=librosa.feature.rms(y=data)\n",
    "    return np.squeeze(rmse)\n",
    "def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n",
    "    mfcc=librosa.feature.mfcc(y=data,sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n",
    "    data = np.squeeze(data).astype(np.float32)\n",
    "    result=np.array([])\n",
    "    result=np.hstack((result,\n",
    "                      zcr(data,frame_length,hop_length),\n",
    "                      rmse(data,frame_length,hop_length),\n",
    "                      mfcc(data,sr,frame_length,hop_length)\n",
    "                     ))\n",
    "    return result\n",
    "\n",
    "def get_features(audio_file, target_sr=22050):\n",
    "    if audio_file is None:\n",
    "        return None\n",
    "    \n",
    "    _, data = audio_file\n",
    "    \n",
    "    # Ensure the data is in floating-point format\n",
    "    if data.dtype != np.float32:\n",
    "        # Convert data to floating-point format and normalize to range [-1, 1]\n",
    "        data = librosa.util.buf_to_float(data)\n",
    "    \n",
    "    # Resample audio data to the target sample rate\n",
    "    data_resampled = librosa.resample(data, orig_sr=_, target_sr=target_sr)\n",
    "    \n",
    "    aud = extract_features(data_resampled)\n",
    "    audio = np.array(aud)\n",
    "    return audio\n",
    "\n",
    "\n",
    "# def predict_emotions(audio_file):\n",
    "#     features = get_features(audio_file)\n",
    "#     prde = pd.DataFrame(features)\n",
    "#     new_data = np.zeros((1, 4158))\n",
    "#     new_data[:, :prde.shape[0]] = prde.T\n",
    "#     prde1 = pd.DataFrame(new_data)\n",
    "#     prde1 = prde1.fillna(0)\n",
    "#     prde1 = np.expand_dims(prde1, axis=2)\n",
    "#     emotions = loaded_model.predict(prde1)\n",
    "#     predictions = encoder.inverse_transform(emotions)\n",
    "#     return predictions\n",
    "\n",
    "def predict_emotions_all(audio_file_1,audio_file_2,audio_file_3,audio_file_4,audio_file_5,audio_file_6,audio_file_7,audio_file_8,audio_file_9):\n",
    "    files = [audio_file_1,audio_file_2,audio_file_3,audio_file_4,audio_file_5,audio_file_6,audio_file_7,audio_file_8,audio_file_9]\n",
    "    predictions = []\n",
    "    for audio_file in files:\n",
    "        features = get_features(audio_file)\n",
    "        prde = pd.DataFrame(features)\n",
    "        new_data = np.zeros((1, 4158))\n",
    "        new_data[:, :prde.shape[0]] = prde.T\n",
    "        prde1 = pd.DataFrame(new_data)\n",
    "        prde1 = prde1.fillna(0)\n",
    "        prde1 = np.expand_dims(prde1, axis=2)\n",
    "        emotions = loaded_model.predict(prde1)\n",
    "        predictions.append(encoder.inverse_transform(emotions)[0][0])\n",
    "    emotion_counts = collections.Counter(predictions)\n",
    "    total_observations = len(predictions)\n",
    "    emotion_percentages = {emotion: count/total_observations*100 for emotion, count in emotion_counts.items()}\n",
    "\n",
    "    headers = ['Emotion', 'Percentage']\n",
    "    table = [[emotion, f\"{percentage:.2f}%\"] for emotion, percentage in emotion_percentages.items()]\n",
    "    table.insert(0, headers)\n",
    "\n",
    "    results = tabulate.tabulate(table, headers='firstrow')\n",
    "    return results\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict_emotions_all,\n",
    "    inputs=audio_df,\n",
    "    outputs=\"text\",\n",
    "    title=\"Emotion Prediction from Audio\",\n",
    "    description=\"Recording audio files and predict the emotions\"\n",
    ")\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_ = gr.Audio(sources=[\"microphone\",\"upload\"], type=\"numpy\", format='wav', max_length=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gradio.components.audio.Audio at 0x2003a167f70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
